{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# Custom Pytorch dataset\n",
    "# Class Dataset, Dataloader\n",
    "# Dataset stores all information regarding dataset (samples, annotations ...)\n",
    "# Dataloader is an iterator wrapper around dataset, that allows to load different samples in real time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "#Our UrbanSoundDataset class inherits from the base Dataset class\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_file, audio_dir):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        \n",
    "    # Len is method we use to define how we use the len syntax.\n",
    "    # len(usd) will return the length of the dataset (number of samples).\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    # getitem is a method we use to define how to get a sample from the dataset.\n",
    "    # work like : a_list[1] -> a_list.__getitem__(1)\n",
    "    # we want to define functions, that will specify, how to get data from dataset.\n",
    "    def __getitem__(self, index):\n",
    "        # Loading waveform of audio sample associated with certain inddex and also the label\n",
    "        # Path to the sample:\n",
    "        # self._get_audio_sample - private methods\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        # Load audio file using torch audio\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        return signal, label\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "        \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8732 samples in the dataset.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No audio I/O backend is available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Pytroch\\2_Custom_Dataset.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m usd \u001b[39m=\u001b[39m UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(usd)\u001b[39m}\u001b[39;00m\u001b[39m samples in the dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m signal, label \u001b[39m=\u001b[39m usd[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m a \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Pytroch\\2_Custom_Dataset.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_label(index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Load audio file using torch audio\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m signal, sr \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(audio_sample_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Pytroch/2_Custom_Dataset.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signal, label\n",
      "File \u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\Lib\\site-packages\\torchaudio\\backend\\no_backend.py:16\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, out, normalization, channels_first, num_frames, offset, filetype)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m      8\u001b[0m     filepath: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m      9\u001b[0m     out: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     filetype: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo audio I/O backend is available.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No audio I/O backend is available."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = \"data/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "    AUDIO_DIR = \"data/UrbanSound8K/audio\"\n",
    "    \n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR)\n",
    "    \n",
    "    print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "    \n",
    "    signal, label = usd[0]\n",
    "    \n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
