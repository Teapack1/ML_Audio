{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SoundFile in d:\\code\\projectspython\\ml_trainingground\\ml_audio\\.venv\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\code\\projectspython\\ml_trainingground\\ml_audio\\.venv\\lib\\site-packages (from SoundFile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\code\\projectspython\\ml_trainingground\\ml_audio\\.venv\\lib\\site-packages (from cffi>=1.0->SoundFile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install SoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Custom Pytorch dataset\n",
    "# Class Dataset, Dataloader\n",
    "# Dataset stores all information regarding dataset (samples, annotations ...)\n",
    "# Dataloader is an iterator wrapper around dataset, that allows to load different samples in real time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "# Constants:\n",
    "\n",
    "ANNOTATIONS_FILE = \"data/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \"data/UrbanSound8K/audio\"\n",
    "\n",
    "# Deciding number of samples that we want to have in our dataset.\n",
    "# We will use 22050 samples, which is 1 second of audio.\n",
    "SAMPLE_RATE = 22050 \n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "# Check if GPU is available, else use cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our UrbanSoundDataset class inherits from the base Dataset class\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 annotations_file,\n",
    "                 audio_dir, \n",
    "                 transformation, \n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        # computing device in use\n",
    "        self.device = device\n",
    "        # Attributes for transformation of audio signal for features like Mel Spectrogram.\n",
    "        # assignt the task to the computing device (cuda)\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    # Len is method we use to define how we use the len syntax.\n",
    "    # len(usd) will return the length of the dataset (number of samples).\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    # getitem is a method we use to define how to get a sample from the dataset.\n",
    "    # work like : a_list[1] -> a_list.__getitem__(1)\n",
    "    # we want to define functions, that will specify, how to get data from dataset.\n",
    "    def __getitem__(self, index):\n",
    "        # Loading waveform of audio sample associated with certain inddex and also the label\n",
    "        # Path to the sample:\n",
    "        # self._get_audio_sample - private methods\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        # Loads particular audio file from dataset class using torch audio.\n",
    "        # signal -> (num_channels, samples) -> (2, 16000) -> (1, 16000)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        # When using CUDA !! Register the signal to the device (.to(self.device)) also the resampler object needs to be registered to the device.):\n",
    "        signal = signal.to(self.device)\n",
    "        # Resample the signal to the target sample rate, so all audio clips will have same sample rate.\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        # Mix the signal down to mono, if it is not already.\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        # Before transformation, the signal should have number of samples = num_samples\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        # Call our mel_spectrogram transformation object on the signal to extract mel spectrograms.\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "    \n",
    "    # signal -> Tensor -> (1, num_samples)\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            # If the condition above is true (i.e., the signal has more samples than allowed), this line trims signal down to have exactly NUM_SAMPLES samples. It does this by keeping all channels (indicated by the first colon :) but only keeping the first NUM_SAMPLES samples along the second dimension (indicated by :NUM_SAMPLES).\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            # If the condition above is true (i.e., the signal has fewer samples than allowed), this line pads the signal with zeros along the second dimension (indicated by the second colon :) until it has exactly NUM_SAMPLES samples.\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            # number of items we want to prepend, number of items we want to append to the tensor. -> padding only along the second (last) dimension.\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            # Pad the signal with zeros along the second dimension.\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            # When using CUDA !! Register the resampler to the device (.to(self.device)) also the loaded audio signal needs to be registered to the device.):\n",
    "            resampler = resampler.to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    # mean operation mix down averages the channels into a mono (single-channel).\n",
    "    # The dim=0 argument tells PyTorch to compute the mean along the channel dimension, and keepdim=True ensures that the result still has two dimensions (i.e., (1, 16000) instead of (16000,)).\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1: # (2, 16000) -> (1, 16000)\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "        \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Mel Spectrograms with Torchaudio\n",
    "### Preprocess audio signal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8732 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    # Torchaudio.transforms - transformations for audio data like MFCCs, MelSpectrogram, AmplitudeToDB, MuLawEncoding, Resample, Spectrogram, etc.\n",
    "    # MelSpectrogram - Torchaudio transformation that takes in a raw audio signal and outputs the mel spectrogram.\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    # ms = mel_spectrogram(signal)\n",
    "    \n",
    "    # Create instance of our UrbanSoundDataset class\n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, \n",
    "                            AUDIO_DIR, \n",
    "                            mel_spectrogram, \n",
    "                            SAMPLE_RATE, \n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "    print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "    # Get the 0th sample\n",
    "    signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
