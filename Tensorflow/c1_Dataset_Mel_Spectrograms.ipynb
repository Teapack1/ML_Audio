{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename, \"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\", sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\", bit_depth_string)[0]\n",
    "        \n",
    "        wave_file.close()\n",
    "\n",
    "        # Load the audio file with librosa\n",
    "        y, sr = librosa.load(filename, sr=None, mono=True)  # Load as mono\n",
    "\n",
    "        # Compute RMS of the audio signal using librosa\n",
    "        # rms = librosa.feature.rms(y=y)[0]\n",
    "        # avg_rms = np.mean(rms)  # Average RMS over time if needed\n",
    "        avg_rms = None\n",
    "        # Compute the length of the audio sample in seconds\n",
    "        length_in_seconds = len(y) / sr  # Total samples / Sample rate\n",
    "        \n",
    "        # Length in samples\n",
    "        length_in_samples = len(y)\n",
    "        \n",
    "        return (num_channels, sample_rate, bit_depth, avg_rms, length_in_seconds, length_in_samples)  # Added length_in_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and install dependencies\n",
    "# tensorflow_io 0.28 is compatible with TensorFlow 2.11\n",
    "# Python 3.10* needed.\n",
    "#! pip install tensorflow==2.11.* tensorflow-io==0.31.0 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and constants\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "import IPython.display as ipd\n",
    "\n",
    "wavfilehelper = WavFileHelper()\n",
    "\n",
    "DATA_DIR = 'D:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\data'\n",
    "METADATA = os.path.join(DATA_DIR, \"UrbanSound8K\", \"metadata\", \"UrbanSound8K.csv\")\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, \"UrbanSound8K\", \"audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Observe the Dataset</i></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO01 = os.path.join(AUDIO_DIR, \"fold1/101415-3-0-2.wav\")\n",
    "AUDIO02 = os.path.join(AUDIO_DIR, \"fold10/2937-1-0-0.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Dataset metadata</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Class distribution</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diversity in sample properties**\n",
    "*  Number of channels  \n",
    "*  Sample rates\n",
    "*  Bit depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiodata = []\n",
    "#Iterrows used to iterate over DataFrame rows as (index, Series) pairs. Each row is returned as a Series object, and you can access the values of the Series to process each row individually.\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(AUDIO_DIR,'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    data = wavfilehelper.read_file_properties(file_name)\n",
    "    audiodata.append(data)\n",
    "    # Convert into a Panda dataframe\n",
    "audiodf = pd.DataFrame(audiodata, columns=['num_channels', 'sample_rate', 'bit_depth', 'avg_rms', 'length_in_seconds', 'length_in_samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiodf.head()\n",
    "\n",
    "# num of channels \n",
    "print(\"Channels: \")\n",
    "print(audiodf.num_channels.value_counts(normalize=True))\n",
    "\n",
    "# sample rates \n",
    "print(\"Sample Rates: \")\n",
    "print(audiodf.sample_rate.value_counts(normalize=True))\n",
    "\n",
    "# bit depth\n",
    "print(\"Bit Depth: \")\n",
    "print(audiodf.bit_depth.value_counts(normalize=True))\n",
    "\n",
    "# length in samples\n",
    "print(\"Samples: \")\n",
    "print(audiodf.length_in_samples.value_counts())\n",
    "\n",
    "# RMS\n",
    "# print(audiodf.avg_rms.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with Librosa\n",
    "* Resample to common sample rate\n",
    "* Bit-depth Normalization\n",
    "* Mix-down to mono channel\n",
    "* Cut the length to 2sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample\n",
    "filename = AUDIO01\n",
    "\n",
    "# Librosa load does resample, mono and bit depth conversion.\n",
    "scipy_sample_rate, scipy_audio = wav.read(filename) \n",
    "librosa_audio, librosa_sample_rate = librosa.load(filename) \n",
    "\n",
    "print('Original sample rate:', scipy_sample_rate) \n",
    "print('Librosa sample rate:', librosa_sample_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bit-depth \n",
    "# also known as bit resolution, refers to the number of bits used to represent each sample in a digital audio file.\n",
    "# Librosaâ€™s load function will also normalise the data so it's values range between -1 and 1. This removes the complication of the dataset having a wide range of bit-depths.\n",
    "\n",
    "print('Original audio file min~max range:', np.min(scipy_audio), 'to', np.max(scipy_audio))\n",
    "print('Librosa audio file min~max range:', np.min(librosa_audio), 'to', np.max(librosa_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix down to mono\n",
    "\n",
    "# Original audio with 2 channels \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(scipy_audio)\n",
    "\n",
    "# Librosa audio with channels merged \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio)\n",
    "\n",
    "print(f\"Scipy audio shape: {scipy_audio.shape}, Librosa audio shape: {librosa_audio.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice for common length of 1 seconds\n",
    "def slice_audio(librosa_audio, librosa_sample_rate = 22050):\n",
    "    SAMPLE_LENGTH = 1 * librosa_sample_rate\n",
    "\n",
    "    librosa_audio_sliced = librosa_audio[:SAMPLE_LENGTH]\n",
    "    if len(librosa_audio) < SAMPLE_LENGTH:\n",
    "        # print(f\"Audio length {len(librosa_audio)} is less than 2 seconds. Padding with zeros.\")\n",
    "        # np.pad specifies the number of values to add at the beginning and the end of the librosa_audio array.\n",
    "        # 0 -> no padding in the beginning.\n",
    "        # SAMPLE_LENGTH - len(librosa_audio) -> number of zeros to end, ensuring the total length is 2 seconds.\n",
    "        librosa_audio_sliced = np.pad(librosa_audio, (0, SAMPLE_LENGTH - len(librosa_audio)), constant_values=0)\n",
    "    return librosa_audio_sliced\n",
    "\n",
    "# print(f\"Librosa audio before: {librosa_audio.shape} and after: {slice_audio(librosa_audio).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=slice_audio(librosa_audio), rate=librosa_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ______________________________________________________________________________\n",
    "## <i>FEATURE EXTRACTION:</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "**Extract Spectrogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(audio_path):\n",
    "    \n",
    "    audio_file, librosa_sample_rate = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "    audio_file = slice_audio(audio_file, librosa_sample_rate)\n",
    "\n",
    "    spectrogram = librosa.stft(audio_file, n_fft=512, win_length=512, dtype=np.float32)\n",
    "    spectrogram = librosa.amplitude_to_db(abs(spectrogram), ref=np.max)\n",
    "    #librosa.display.specshow(spectrogram, sr=librosa_sample_rate, x_axis='time')\n",
    "\n",
    "    # spectrogram = tf.expand_dims(spectrogram, axis = 2)\n",
    "\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Fourier transform\n",
    "#In each iteration of the loop, the variable index is assigned the index value of the current row, and the variable row is assigned the data of the current row (as a Series object).\n",
    "row = df.iloc[99]\n",
    "\n",
    "file = os.path.join(AUDIO_DIR, \"fold\" + str(row[\"fold\"]) + \"\\\\\" + str(row[\"slice_file_name\"]))\n",
    "audio_file, librosa_sample_rate = librosa.load(file, res_type='kaiser_fast')\n",
    "audio_file = slice_audio(audio_file, librosa_sample_rate)\n",
    "\n",
    "spectrogram = librosa.stft(audio_file, n_fft=512, win_length=512)\n",
    "spectrogram = librosa.amplitude_to_db(abs(spectrogram), ref=np.max)\n",
    "librosa.display.specshow(spectrogram, sr=librosa_sample_rate, x_axis='time')\n",
    "print(spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "#In each iteration of the loop, the variable index is assigned the index value of the current row, and the variable row is assigned the data of the current row (as a Series object).\n",
    "for index, row in df.iterrows():\n",
    "    file = os.path.join(AUDIO_DIR, \"fold\" + str(row[\"fold\"]) + \"\\\\\" + str(row[\"slice_file_name\"]))\n",
    "    class_label = row[\"class\"]\n",
    "    data = extract_spectrogram(file)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "    \n",
    "featuresdf = pd.DataFrame(features, columns=[\"features\", \"class_label\"])\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf.iloc[0][\"features\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>END OF FEATURE EXTRACTION</i>\n",
    "# ______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the data and labels**\n",
    "<p>We will use sklearn.preprocessing.LabelEncoder to encode the categorical text data into model-understandable numerical data.</p>\n",
    "<p><i>Meaning, that every class has a column, which is either 0 or 1</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.features.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Store the preprocessed data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store x_train \n",
    "%store x_test \n",
    "%store y_train \n",
    "%store y_test \n",
    "%store yy \n",
    "%store le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
