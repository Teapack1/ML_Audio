{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the preprocessed data from previous notebook\n",
    "\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train \n",
    "%store -r y_test \n",
    "%store -r yy \n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "\n",
    "num_rows = 257\n",
    "num_columns = 345\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the Model**\n",
    "<p>Loss function - we will use categorical_crossentropy. This is the most common choice for classification. A lower score indicates that the model is performing better.</p>\n",
    "<p>Metrics - we will use the accuracy metric which will allow us to view the accuracy score on the validation data when we train the model.</p>\n",
    "<p>Optimizer - here we will use adam which is a generally good optimizer for many use cases.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 344, 16)      80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 128, 172, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 172, 16)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 127, 171, 32)      2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 63, 85, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 63, 85, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 62, 84, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 42, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 31, 42, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 30, 41, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 15, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 15, 20, 128)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "55/55 [==============================] - 10s 145ms/step - loss: 6.4802 - accuracy: 0.1191\n",
      "Pre-training accuracy: 11.9061%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.3086 - accuracy: 0.1170\n",
      "Epoch 1: val_loss improved from inf to 2.34234, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 167s 6s/step - loss: 3.3086 - accuracy: 0.1170 - val_loss: 2.3423 - val_accuracy: 0.1580\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.2357 - accuracy: 0.1609\n",
      "Epoch 2: val_loss improved from 2.34234 to 2.29940, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 2.2357 - accuracy: 0.1609 - val_loss: 2.2994 - val_accuracy: 0.1059\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.0461 - accuracy: 0.2285\n",
      "Epoch 3: val_loss improved from 2.29940 to 2.18119, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 152s 5s/step - loss: 2.0461 - accuracy: 0.2285 - val_loss: 2.1812 - val_accuracy: 0.1362\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9882 - accuracy: 0.2498\n",
      "Epoch 4: val_loss did not improve from 2.18119\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.9882 - accuracy: 0.2498 - val_loss: 2.2014 - val_accuracy: 0.1122\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9649 - accuracy: 0.2531\n",
      "Epoch 5: val_loss improved from 2.18119 to 2.15937, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 152s 5s/step - loss: 1.9649 - accuracy: 0.2531 - val_loss: 2.1594 - val_accuracy: 0.1763\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9476 - accuracy: 0.2691\n",
      "Epoch 6: val_loss improved from 2.15937 to 2.14654, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.9476 - accuracy: 0.2691 - val_loss: 2.1465 - val_accuracy: 0.1614\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9212 - accuracy: 0.2724\n",
      "Epoch 7: val_loss improved from 2.14654 to 2.13303, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 153s 5s/step - loss: 1.9212 - accuracy: 0.2724 - val_loss: 2.1330 - val_accuracy: 0.1769\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9164 - accuracy: 0.2819\n",
      "Epoch 8: val_loss improved from 2.13303 to 2.10427, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 150s 5s/step - loss: 1.9164 - accuracy: 0.2819 - val_loss: 2.1043 - val_accuracy: 0.2015\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8853 - accuracy: 0.2942\n",
      "Epoch 9: val_loss improved from 2.10427 to 2.06669, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.8853 - accuracy: 0.2942 - val_loss: 2.0667 - val_accuracy: 0.2003\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8616 - accuracy: 0.3128\n",
      "Epoch 10: val_loss improved from 2.06669 to 2.02131, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 150s 5s/step - loss: 1.8616 - accuracy: 0.3128 - val_loss: 2.0213 - val_accuracy: 0.2484\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8134 - accuracy: 0.3450\n",
      "Epoch 11: val_loss improved from 2.02131 to 1.96694, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.8134 - accuracy: 0.3450 - val_loss: 1.9669 - val_accuracy: 0.2690\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.7582 - accuracy: 0.3609\n",
      "Epoch 12: val_loss improved from 1.96694 to 1.89065, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.7582 - accuracy: 0.3609 - val_loss: 1.8906 - val_accuracy: 0.2925\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6888 - accuracy: 0.3901\n",
      "Epoch 13: val_loss improved from 1.89065 to 1.80866, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.6888 - accuracy: 0.3901 - val_loss: 1.8087 - val_accuracy: 0.3286\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6410 - accuracy: 0.4064\n",
      "Epoch 14: val_loss did not improve from 1.80866\n",
      "28/28 [==============================] - 150s 5s/step - loss: 1.6410 - accuracy: 0.4064 - val_loss: 1.8623 - val_accuracy: 0.3074\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5669 - accuracy: 0.4332\n",
      "Epoch 15: val_loss improved from 1.80866 to 1.79541, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.5669 - accuracy: 0.4332 - val_loss: 1.7954 - val_accuracy: 0.3183\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5575 - accuracy: 0.4282\n",
      "Epoch 16: val_loss improved from 1.79541 to 1.72904, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 150s 5s/step - loss: 1.5575 - accuracy: 0.4282 - val_loss: 1.7290 - val_accuracy: 0.3366\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5170 - accuracy: 0.4474\n",
      "Epoch 17: val_loss did not improve from 1.72904\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.5170 - accuracy: 0.4474 - val_loss: 1.7630 - val_accuracy: 0.3383\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4924 - accuracy: 0.4598\n",
      "Epoch 18: val_loss did not improve from 1.72904\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.4924 - accuracy: 0.4598 - val_loss: 1.7461 - val_accuracy: 0.3606\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4538 - accuracy: 0.4747\n",
      "Epoch 19: val_loss did not improve from 1.72904\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.4538 - accuracy: 0.4747 - val_loss: 1.7544 - val_accuracy: 0.3537\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4737 - accuracy: 0.4697\n",
      "Epoch 20: val_loss did not improve from 1.72904\n",
      "28/28 [==============================] - 151s 5s/step - loss: 1.4737 - accuracy: 0.4697 - val_loss: 1.7324 - val_accuracy: 0.3572\n",
      "Training completed in time:  0:50:40.865313\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 20\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.3550465404987335\n",
      "Testing Accuracy:  0.3571837544441223\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/Spectrogram_Classification_Model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "\n",
    "# Slice for common length of 2 seconds\n",
    "def slice_audio(librosa_audio, librosa_sample_rate = 22050):\n",
    "    SAMPLE_LENGTH = 2 * librosa_sample_rate\n",
    "\n",
    "    librosa_audio_sliced = librosa_audio[:SAMPLE_LENGTH]\n",
    "    if len(librosa_audio) < SAMPLE_LENGTH:\n",
    "        # print(f\"Audio length {len(librosa_audio)} is less than 2 seconds. Padding with zeros.\")\n",
    "        # np.pad specifies the number of values to add at the beginning and the end of the librosa_audio array.\n",
    "        # 0 -> no padding in the beginning.\n",
    "        # SAMPLE_LENGTH - len(librosa_audio) -> number of zeros to end, ensuring the total length is 2 seconds.\n",
    "        librosa_audio_sliced = np.pad(librosa_audio, (0, SAMPLE_LENGTH - len(librosa_audio)), constant_values=0)\n",
    "    return librosa_audio_sliced\n",
    "\n",
    "\n",
    "def extract_spectrogram(audio_path):\n",
    "    \n",
    "    audio_file, librosa_sample_rate = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "    audio_file = slice_audio(audio_file, librosa_sample_rate)\n",
    "\n",
    "    spectrogram = librosa.stft(audio_file, n_fft=512, win_length=512, dtype=np.float32)\n",
    "    spectrogram = librosa.amplitude_to_db(abs(spectrogram), ref=np.max)\n",
    "    #librosa.display.specshow(spectrogram, sr=librosa_sample_rate, x_axis='time')\n",
    "\n",
    "    # spectrogram = tf.expand_dims(spectrogram, axis = 2)\n",
    "\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_spectrogram(file_name)\n",
    "    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "    predicted_vector = np.argmax(model.predict(prediction_feature), axis=-1)\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    predicted_proba_vector = model.predict(prediction_feature) \n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation**\n",
    "<p>On new samples<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('saved_models/Spectrogram_Classification_Model.keras')\n",
    "\n",
    "VAL_DIR = \"D:\\\\Code\\\\ProjectsPython\\\\ML_TrainingGround\\\\ML_Audio\\\\data\\\\UrbanSound8K\\\\validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 638ms/step\n",
      "The predicted class is: engine_idling \n",
      "\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "air_conditioner \t\t :  0.24355262517929077148437500000000\n",
      "car_horn \t\t :  0.01309690158814191818237304687500\n",
      "children_playing \t\t :  0.01324258930981159210205078125000\n",
      "dog_bark \t\t :  0.01002905238419771194458007812500\n",
      "drilling \t\t :  0.02467735111713409423828125000000\n",
      "engine_idling \t\t :  0.48600000143051147460937500000000\n",
      "gun_shot \t\t :  0.01205831021070480346679687500000\n",
      "jackhammer \t\t :  0.07980767637491226196289062500000\n",
      "siren \t\t :  0.06876901537179946899414062500000\n",
      "street_music \t\t :  0.04876649007201194763183593750000\n"
     ]
    }
   ],
   "source": [
    "# Class: Air Conditioner\n",
    "filename = os.path.join(VAL_DIR, \"air_conditioner.mp3\")\n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "The predicted class is: engine_idling \n",
      "\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "air_conditioner \t\t :  0.19074751436710357666015625000000\n",
      "car_horn \t\t :  0.00494202831760048866271972656250\n",
      "children_playing \t\t :  0.01620393991470336914062500000000\n",
      "dog_bark \t\t :  0.00649807881563901901245117187500\n",
      "drilling \t\t :  0.01107691135257482528686523437500\n",
      "engine_idling \t\t :  0.62436318397521972656250000000000\n",
      "gun_shot \t\t :  0.01037050131708383560180664062500\n",
      "jackhammer \t\t :  0.05050666257739067077636718750000\n",
      "siren \t\t :  0.06733405590057373046875000000000\n",
      "street_music \t\t :  0.01795705407857894897460937500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: Car idle\n",
    "filename = os.path.join(VAL_DIR, \"car_idle.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "The predicted class is: siren \n",
      "\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "air_conditioner \t\t :  0.04661423712968826293945312500000\n",
      "car_horn \t\t :  0.08865276724100112915039062500000\n",
      "children_playing \t\t :  0.11344639956951141357421875000000\n",
      "dog_bark \t\t :  0.13278207182884216308593750000000\n",
      "drilling \t\t :  0.08890788257122039794921875000000\n",
      "engine_idling \t\t :  0.04933480918407440185546875000000\n",
      "gun_shot \t\t :  0.03988853842020034790039062500000\n",
      "jackhammer \t\t :  0.06437175720930099487304687500000\n",
      "siren \t\t :  0.24432623386383056640625000000000\n",
      "street_music \t\t :  0.13167531788349151611328125000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: dog bark\n",
    "filename = os.path.join(VAL_DIR, \"dog_barking.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "The predicted class is: siren \n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "air_conditioner \t\t :  0.00457365065813064575195312500000\n",
      "car_horn \t\t :  0.01387429982423782348632812500000\n",
      "children_playing \t\t :  0.00199420424178242683410644531250\n",
      "dog_bark \t\t :  0.00151245389133691787719726562500\n",
      "drilling \t\t :  0.41210639476776123046875000000000\n",
      "engine_idling \t\t :  0.00198868243023753166198730468750\n",
      "gun_shot \t\t :  0.00001093672017304925248026847839\n",
      "jackhammer \t\t :  0.03502830117940902709960937500000\n",
      "siren \t\t :  0.52517592906951904296875000000000\n",
      "street_music \t\t :  0.00373518071137368679046630859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: drill\n",
    "filename = os.path.join(VAL_DIR, \"drill.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "The predicted class is: siren \n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "air_conditioner \t\t :  0.02664410322904586791992187500000\n",
      "car_horn \t\t :  0.02038235776126384735107421875000\n",
      "children_playing \t\t :  0.06551940739154815673828125000000\n",
      "dog_bark \t\t :  0.04880893230438232421875000000000\n",
      "drilling \t\t :  0.16975186765193939208984375000000\n",
      "engine_idling \t\t :  0.04078878834843635559082031250000\n",
      "gun_shot \t\t :  0.00293349893763661384582519531250\n",
      "jackhammer \t\t :  0.23104561865329742431640625000000\n",
      "siren \t\t :  0.36358994245529174804687500000000\n",
      "street_music \t\t :  0.03053554147481918334960937500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: jackhammer\n",
    "filename = os.path.join(VAL_DIR, \"jackhammer.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "The predicted class is: siren \n",
      "\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_conditioner \t\t :  0.00301683344878256320953369140625\n",
      "car_horn \t\t :  0.10142213106155395507812500000000\n",
      "children_playing \t\t :  0.11878542602062225341796875000000\n",
      "dog_bark \t\t :  0.04199254512786865234375000000000\n",
      "drilling \t\t :  0.03343436121940612792968750000000\n",
      "engine_idling \t\t :  0.00188155565410852432250976562500\n",
      "gun_shot \t\t :  0.00051945296581834554672241210938\n",
      "jackhammer \t\t :  0.01036619301885366439819335937500\n",
      "siren \t\t :  0.54443687200546264648437500000000\n",
      "street_music \t\t :  0.14414460957050323486328125000000\n"
     ]
    }
   ],
   "source": [
    "# Class: kids playing\n",
    "filename = os.path.join(VAL_DIR, \"kids_playing.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "The predicted class is: siren \n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "air_conditioner \t\t :  0.00001835819057305343449115753174\n",
      "car_horn \t\t :  0.02222104184329509735107421875000\n",
      "children_playing \t\t :  0.00399238662794232368469238281250\n",
      "dog_bark \t\t :  0.00138486456125974655151367187500\n",
      "drilling \t\t :  0.00384465279057621955871582031250\n",
      "engine_idling \t\t :  0.00001672653343121055513620376587\n",
      "gun_shot \t\t :  0.00000098241787327424390241503716\n",
      "jackhammer \t\t :  0.00066541740670800209045410156250\n",
      "siren \t\t :  0.94724416732788085937500000000000\n",
      "street_music \t\t :  0.02061129733920097351074218750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: siren\n",
    "filename = os.path.join(VAL_DIR, \"siren.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "The predicted class is: street_music \n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "air_conditioner \t\t :  0.04146852344274520874023437500000\n",
      "car_horn \t\t :  0.09178592264652252197265625000000\n",
      "children_playing \t\t :  0.04445578157901763916015625000000\n",
      "dog_bark \t\t :  0.02995055913925170898437500000000\n",
      "drilling \t\t :  0.04206641390919685363769531250000\n",
      "engine_idling \t\t :  0.03835495933890342712402343750000\n",
      "gun_shot \t\t :  0.00504108518362045288085937500000\n",
      "jackhammer \t\t :  0.05032153427600860595703125000000\n",
      "siren \t\t :  0.11103385686874389648437500000000\n",
      "street_music \t\t :  0.54552137851715087890625000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:362: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:366: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    }
   ],
   "source": [
    "# Class: street music\n",
    "filename = os.path.join(VAL_DIR, \"street_music.mp3\")\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
