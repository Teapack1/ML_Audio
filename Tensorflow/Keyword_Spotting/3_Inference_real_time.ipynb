{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword spoting Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "MODEL_PATH = \"D:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Tensorflow\\saved_models\\mfcc_Classification_Model.keras\"\n",
    "# A better way for of storing constants for the ML system \n",
    "# is having an external confix file for consistency purposes.\n",
    "NUM_SAMPLES_TO_CONSIDER = 22050 # 1 sec worth of sound in librosa\n",
    "CONFIDENCE_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singleton class - a calss that only can have one instance in the srvice.\n",
    "class _Keyword_Spotting_Service:\n",
    "    \n",
    "    model = None\n",
    "    #Mapping taken from the dataset json file\n",
    "    _mappings = [\n",
    "        \"five\",\n",
    "        \"four\",\n",
    "        \"go\",\n",
    "        \"no\",\n",
    "        \"off\",\n",
    "        \"on\",\n",
    "        \"one\",\n",
    "        \"stop\",\n",
    "        \"three\",\n",
    "        \"tree\",\n",
    "        \"two\",\n",
    "        \"wow\",\n",
    "        \"yes\",\n",
    "        \"_background_noise_\"\n",
    "    ]\n",
    "    \n",
    "    # Instance of the class - PYthon does not enforce Singleton class, to we need to do it manually\n",
    "    _instance = None\n",
    "    \n",
    "    def predict(self, file_path):\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        MFCCs = self.preprocess(file_path) # (#(44) segments, #(13) coefficients)\n",
    "        \n",
    "         # Convert 2d MFCCs arry into 4d array -> (# samples, # segments, # coefficients, # channels)\n",
    "        MFCCs = MFCCs[np.newaxis, ..., np.newaxis]\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = self.model.predict(MFCCs) # [ [0.1, 0.6, 0.1, 0.2] ]\n",
    "        predicted_index = np.argmax(predictions) # 1\n",
    "        predicted_keyword = self._mappings[predicted_index]\n",
    "        \n",
    "        return predicted_keyword\n",
    "        \n",
    "    def preprocess(self, file_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "        \n",
    "        # Load audio file\n",
    "        signal, sr = librosa.load(file_path)\n",
    "        \n",
    "        # Ensure consistency in the audio file length\n",
    "        if len(signal) > NUM_SAMPLES_TO_CONSIDER:\n",
    "            signal = signal[:NUM_SAMPLES_TO_CONSIDER]\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        MFCCs = librosa.feature.mfcc(y = signal, n_mfcc = n_mfcc, n_fft = n_fft, hop_length = hop_length)\n",
    "\n",
    "        # Transpose the matrix\n",
    "        return MFCCs.T\n",
    "    \n",
    "    def listen_and_predict(self, duration=1, sr=22050, overlap=0.5):\n",
    "            buffer = np.zeros(int(sr * duration))\n",
    "            try:\n",
    "                with sd.InputStream(samplerate=sr, channels=1) as stream:\n",
    "                    print(\"Listening... Press Ctrl+C to stop.\")\n",
    "                    while True:\n",
    "                        audio_chunk, overflowed = stream.read(int(sr * overlap))\n",
    "                        buffer = np.concatenate((buffer[len(audio_chunk):], audio_chunk.flatten()))\n",
    "                        keyword = self.predict_chunk(buffer, sr)\n",
    "                        if keyword:\n",
    "                            print(f\"Predicted Keyword: {keyword}\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Stopped listening.\")\n",
    "                pass\n",
    "\n",
    "    def predict_chunk(self, audio_chunk, sr):\n",
    "        if len(audio_chunk) > NUM_SAMPLES_TO_CONSIDER:\n",
    "            audio_chunk = audio_chunk[-NUM_SAMPLES_TO_CONSIDER:]\n",
    "        MFCCs = librosa.feature.mfcc(y=audio_chunk, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
    "        MFCCs = MFCCs.T\n",
    "        MFCCs = MFCCs[np.newaxis, ..., np.newaxis]\n",
    "        predictions = self.model.predict(MFCCs)\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        confidence = predictions[0][predicted_index]\n",
    "        if confidence > CONFIDENCE_THRESHOLD:\n",
    "            predicted_keyword = self._mappings[predicted_index]\n",
    "            return predicted_keyword\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Press Ctrl+C to stop.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 40, 63, 1), found shape=(None, 44, 13, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Tensorflow\\Keyword_Spotting\\3_Inference_real_time.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     kss \u001b[39m=\u001b[39m Keyword_Spotting_Service()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     kss\u001b[39m.\u001b[39;49mlisten_and_predict()\n",
      "\u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Tensorflow\\Keyword_Spotting\\3_Inference_real_time.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m audio_chunk, overflowed \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mread(\u001b[39mint\u001b[39m(sr \u001b[39m*\u001b[39m overlap))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m buffer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((buffer[\u001b[39mlen\u001b[39m(audio_chunk):], audio_chunk\u001b[39m.\u001b[39mflatten()))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m keyword \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_chunk(buffer, sr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m keyword:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted Keyword: \u001b[39m\u001b[39m{\u001b[39;00mkeyword\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\Tensorflow\\Keyword_Spotting\\3_Inference_real_time.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m MFCCs \u001b[39m=\u001b[39m MFCCs\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m MFCCs \u001b[39m=\u001b[39m MFCCs[np\u001b[39m.\u001b[39mnewaxis, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(MFCCs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m predicted_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ProjectsPython/ML_TrainingGround/ML_Audio/Tensorflow/Keyword_Spotting/3_Inference_real_time.ipynb#W3sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m confidence \u001b[39m=\u001b[39m predictions[\u001b[39m0\u001b[39m][predicted_index]\n",
      "File \u001b[1;32md:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3m6hxwgq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\Code\\ProjectsPython\\ML_TrainingGround\\ML_Audio\\.venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 40, 63, 1), found shape=(None, 44, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "def Keyword_Spotting_Service():\n",
    "    # Ensure only one instance of KSS is created\n",
    "    if _Keyword_Spotting_Service._instance is None:\n",
    "        _Keyword_Spotting_Service._instance = _Keyword_Spotting_Service()\n",
    "        _Keyword_Spotting_Service.model = keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "    return _Keyword_Spotting_Service._instance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    kss = Keyword_Spotting_Service()\n",
    "    \n",
    "    kss.listen_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
